{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import time\n",
    "\n",
    "# hack to allow relative imports on Linux within 'celltypes' project\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "# torch defaults\n",
    "torch.set_default_tensor_type('torch.DoubleTensor')\n",
    "    \n",
    "from singlecell.singlecell_simsetup import singlecell_simsetup\n",
    "from singlecell.singlecell_simulate import singlecell_sim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### System checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7.15rc1 (default, Nov 12 2018, 14:31:15) \n",
      "[GCC 7.3.0]\n",
      "1547316995.31\n"
     ]
    }
   ],
   "source": [
    "print sys.version\n",
    "print time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.cuda.current_device()     -  0\n",
      "torch.cuda.device(0)            -  <torch.cuda.device object at 0x7f9a740bbc50>\n",
      "torch.cuda.device_count()       -  1\n",
      "torch.cuda.get_device_name(0)   -  GeForce GTX 1080\n",
      "torch.cuda.is_available()       -  True\n",
      "torch.cuda.memory_allocated()   -  125.024768 MB\n",
      "torch.cuda.memory_cached()      -  288.620544 MB\n"
     ]
    }
   ],
   "source": [
    "# check cuda is working...\n",
    "print \"torch.cuda.current_device()     - \", torch.cuda.current_device()\n",
    "print \"torch.cuda.device(0)            - \", torch.cuda.device(0)\n",
    "print \"torch.cuda.device_count()       - \", torch.cuda.device_count()\n",
    "print \"torch.cuda.get_device_name(0)   - \", torch.cuda.get_device_name(0)\n",
    "print \"torch.cuda.is_available()       - \", torch.cuda.is_available()\n",
    "print \"torch.cuda.memory_allocated()   - \", torch.cuda.memory_allocated() * 1e-6, 'MB'\n",
    "print \"torch.cuda.memory_cached()      - \", torch.cuda.memory_cached() * 1e-6, 'MB'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simsetup on CPU + naive GPU copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading npz of arr genes cells at /media/homes/msmart/Development/repos/biomodels/celltypes/input/memories/2018_scmca_mems_genes_types_boolean_compressed_pruned_A_TFonly.npz ...\n",
      "loaded arr, genes, cells: (2218, 98) (2218,) (98,)\n",
      "Note network method for interaction_matrix() is projection\n"
     ]
    }
   ],
   "source": [
    "simsetup = singlecell_simsetup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2218, 2218)\n",
      "Init tensor mem on GPU (MB): 125.024768\n",
      "Final tensor mem on GPU (MB): 166.38976\n",
      "(2218, 2218)\n",
      "<class 'torch.Tensor'> torch.Size([2218, 2218]) True\n",
      "<class 'torch.Tensor'> torch.Size([2218, 2218]) False\n"
     ]
    }
   ],
   "source": [
    "# TODO maybe add the gpu variants to the vanilla simsetup script, AND add a gpu flag to simsetup?\n",
    "def copy_simsetup_arrays_to_gpu(simsetup, verbose=True):\n",
    "    if verbose:\n",
    "        print 'Init tensor mem on GPU (MB):', torch.cuda.memory_allocated() * 1e-6\n",
    "    simsetup['gpu_J'] = torch.from_numpy(simsetup['J']).cuda()\n",
    "    simsetup['gpu_XI'] = torch.from_numpy(simsetup['XI']).cuda()\n",
    "    simsetup['gpu_AINV'] = torch.from_numpy(simsetup['A_INV']).cuda()\n",
    "    if verbose:\n",
    "        print 'Final tensor mem on GPU (MB):', torch.cuda.memory_allocated() * 1e-6\n",
    "    return simsetup\n",
    "\n",
    "print simsetup['J'].shape\n",
    "simsetup = copy_simsetup_arrays_to_gpu(simsetup)\n",
    "print simsetup['J'].shape  # cpu numpy J and torch gpu J both in simsetup now\n",
    "print type(simsetup['gpu_J']), simsetup['gpu_J'].shape, simsetup['gpu_J'].is_cuda\n",
    "print type(simsetup['gpu_J'].cpu()), simsetup['gpu_J'].cpu().shape, simsetup['gpu_J'].cpu().is_cuda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Singlecell basic simulation: CPU vs GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Arg 'level' is None -- setting field level to 'level_1'\n",
      "cell steps: 0  H(state) = -1059.9999999999943\n",
      "cell steps: 1  H(state) = -974.2196935041254\n",
      "cell steps: 2  H(state) = -960.9644347041556\n",
      "cell steps: 3  H(state) = -988.3584948319874\n",
      "cell steps: 4  H(state) = -992.9954416400026\n",
      "cell steps: 5  H(state) = -996.4359518841029\n",
      "cell steps: 6  H(state) = -998.6586718111878\n",
      "cell steps: 7  H(state) = -991.6673165119836\n",
      "cell steps: 8  H(state) = -982.7363902064856\n",
      "cell steps: 9  H(state) = -1007.743858488006\n",
      "cell steps: 10  H(state) = -961.9471730608138\n",
      "cell steps: 11  H(state) = -978.2490788581464\n",
      "cell steps: 12  H(state) = -997.9018155039371\n",
      "cell steps: 13  H(state) = -988.0143629462646\n",
      "cell steps: 14  H(state) = -999.6240145299898\n",
      "cell steps: 15  H(state) = -972.2613485073005\n",
      "cell steps: 16  H(state) = -1011.6976157578556\n",
      "cell steps: 17  H(state) = -1001.9636820507197\n",
      "cell steps: 18  H(state) = -997.026634931203\n",
      "cell steps: 19  H(state) = -976.1731057783068\n",
      "cell steps: 20  H(state) = -978.6644211110868\n",
      "cell steps: 21  H(state) = -969.8821013896134\n",
      "cell steps: 22  H(state) = -982.1489327326905\n",
      "cell steps: 23  H(state) = -971.9021344692267\n",
      "cell steps: 24  H(state) = -1007.2857609143946\n",
      "cell steps: 25  H(state) = -995.3502458195383\n",
      "cell steps: 26  H(state) = -980.2629765025928\n",
      "cell steps: 27  H(state) = -992.5000358570352\n",
      "cell steps: 28  H(state) = -1005.0352298241212\n",
      "cell steps: 29  H(state) = -1008.0816032829631\n",
      "cell steps: 30  H(state) = -994.4856089877595\n",
      "cell steps: 31  H(state) = -1000.0195279470543\n",
      "cell steps: 32  H(state) = -983.8600705813185\n",
      "cell steps: 33  H(state) = -1010.8870372913343\n",
      "cell steps: 34  H(state) = -973.5056399052555\n",
      "cell steps: 35  H(state) = -994.5273018464225\n",
      "cell steps: 36  H(state) = -980.8045442482678\n",
      "cell steps: 37  H(state) = -987.4773630063178\n",
      "cell steps: 38  H(state) = -995.7572140557327\n",
      "cell steps: 39  H(state) = -960.651194070103\n",
      "cell steps: 40  H(state) = -980.9814718452701\n",
      "cell steps: 41  H(state) = -984.9467403893557\n",
      "cell steps: 42  H(state) = -970.5217836704435\n",
      "cell steps: 43  H(state) = -972.5211026726819\n",
      "cell steps: 44  H(state) = -987.5143742238672\n",
      "cell steps: 45  H(state) = -988.4657353064161\n",
      "cell steps: 46  H(state) = -1004.8012775653606\n",
      "cell steps: 47  H(state) = -970.0983323429632\n",
      "cell steps: 48  H(state) = -1006.768573229293\n",
      "[-1. -1. -1. ...  1.  1.  1.]\n",
      "Writing state to file..\n",
      "/media/homes/msmart/Development/repos/biomodels/celltypes/singlecell/runs/2019-01-12 01.16.46PM\n",
      "Done\n",
      "CPU timer: 12.5117371082\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "singlecell_sim(init_id='Macrophage (A)', field_protocol=None, plot_period=10,\n",
    "               iterations=50, simsetup=simsetup, flag_write=True, beta=2.2)\n",
    "deltat = time.time() - t0\n",
    "print \"CPU timer:\", deltat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "from singlecell.singlecell_class import Cell\n",
    "from singlecell.singlecell_constants import NUM_FULL_STEPS, BURST_ERROR_PERIOD, APP_FIELD_STRENGTH, EXT_FIELD_STRENGTH, \\\n",
    "                                            BETA, ASYNC_BATCH\n",
    "from singlecell.singlecell_data_io import run_subdir_setup, runinfo_append\n",
    "from singlecell.singlecell_fields import field_setup\n",
    "from singlecell.singlecell_simsetup import singlecell_simsetup, unpack_simsetup\n",
    "\n",
    "from random import shuffle\n",
    "\n",
    "\n",
    "# NOTE THIS IS THE ONLY THING THAT CHANGED -- benchmark vs deep if gpu else cpu just for this?\n",
    "def gpu_internal_field(state, gene_idx, t, intxn_matrix):\n",
    "    \"\"\"\n",
    "    Original slow summation:\n",
    "    h_1 = 0\n",
    "    intxn_list = range(0, gene_idx) + range(gene_idx+1, N)\n",
    "    for j in intxn_list:\n",
    "        h_1 += J[gene_idx,j] * state[j,t]  # plus some other field terms... do we care for these?\n",
    "    \"\"\"\n",
    "    # move state to gpu\n",
    "    gpu_state_vec_at_t = torch.from_numpy(state[:,t]).cuda()\n",
    "    # compute\n",
    "    gpu_internal_field = torch.dot(intxn_matrix[gene_idx,:], gpu_state_vec_at_t)  # note diagonals assumed to be zero (enforce in J definition)\n",
    "    # send scalar back\n",
    "    internal_field = gpu_internal_field.item()\n",
    "    #internal_field = gpu_internal_field.cpu()\n",
    "    #print type(internal_field), internal_field\n",
    "    return internal_field\n",
    "\n",
    "\n",
    "def gpu_glauber_dynamics_update(state, gene_idx, t, intxn_matrix, unirand, beta=BETA, ext_field=None, app_field=None,\n",
    "                                ext_field_strength=EXT_FIELD_STRENGTH, app_field_strength=APP_FIELD_STRENGTH):\n",
    "    \"\"\"\n",
    "    unirand: pass a uniform 0,1 random number\n",
    "        - note previously unirand = random() OR unirand = np.random_intel.random() from intel python distribution\n",
    "    See page 107-111 Amit for discussion on functional form\n",
    "    ext_field - N x 1 - field external to the cell in a signalling sense; exosome field in multicell sym\n",
    "    ext_field_strength  - scaling factor for ext_field\n",
    "    app_field - N x 1 - unnatural external field (e.g. force TF on for some time period experimentally)\n",
    "    app_field_strength - scaling factor for appt_field\n",
    "    \"\"\"\n",
    "    assert intxn_matrix.is_cuda\n",
    "    total_field = gpu_internal_field(state, gene_idx, t, intxn_matrix)\n",
    "    if ext_field is not None:\n",
    "        total_field += ext_field_strength * ext_field[gene_idx]\n",
    "    if app_field is not None:\n",
    "        total_field += app_field_strength * app_field[gene_idx]\n",
    "    prob_on_after_timestep = 1 / (1 + np.exp(-2*beta*total_field))  # probability that site i will be \"up\" after the timestep\n",
    "    #print  \"PRE state[gene_idx, t]\", t, state[gene_idx, t], unirand, prob_on_after_timestep\n",
    "    if prob_on_after_timestep > unirand:\n",
    "        state[gene_idx, t] = 1.0\n",
    "    else:\n",
    "        state[gene_idx, t] = -1.0\n",
    "    #print  \"POST state[gene_idx, t]\", t, state[gene_idx, t], unirand, total_field\n",
    "    return state\n",
    "\n",
    "\n",
    "# TODO make class method in singlecell\n",
    "def gpu_update_state(singlecell, intxn_matrix, ext_field=None, ext_field_strength=EXT_FIELD_STRENGTH, \n",
    "                     beta=BETA, app_field=None, app_field_strength=APP_FIELD_STRENGTH, async_batch=ASYNC_BATCH):\n",
    "    \"\"\"\n",
    "    async_batch: if True, sample from 0 to N with replacement, else each step will be 'fully random'\n",
    "                 i.e. can update same site twice in a row, vs time gap of at least N substeps\n",
    "                 these produce different short term behaviour, but should reach same steady state\n",
    "    ext_field - N x 1 - field external to the cell in a signalling sense; exosome field in multicell sym\n",
    "    ext_field_strength  - scaling factor for ext_field\n",
    "    app_field - N x 1 - unnatural external field (e.g. force TF on for some time period experimentally)\n",
    "    app_field_strength - scaling factor for appt_field\n",
    "    \"\"\"\n",
    "    assert intxn_matrix.is_cuda\n",
    "\n",
    "    sites = range(singlecell.N)\n",
    "    rsamples = np.random.rand(singlecell.N)  # optimized: pass one to each of the N single spin update calls  TODO: benchmark vs intels\n",
    "    if async_batch:\n",
    "        shuffle(sites)  # randomize site ordering each timestep updates\n",
    "    else:\n",
    "        #sites = np.random.choice(self.N, self.N, replace=True)\n",
    "        #sites = [int(self.N*np.random.random()) for _ in xrange(self.N)]  # this should be same and faster\n",
    "        sites = [int(singlecell.N * u) for u in np.random.rand(singlecell.N)]  # this should be 5-10% percent faster\n",
    "\n",
    "    state_array_ext = np.zeros((singlecell.N, np.shape(singlecell.state_array)[1] + 1))\n",
    "    state_array_ext[:, :-1] = singlecell.state_array  # TODO: make sure don't need array copy\n",
    "    state_array_ext[:,-1] = singlecell.state_array[:,-1]\n",
    "    for idx, site in enumerate(sites):          # TODO: parallelize approximation\n",
    "        #print \"PRE A\", singlecell.steps + 1, state_array_ext[site, singlecell.steps + 1]\n",
    "        state_array_ext = gpu_glauber_dynamics_update(state_array_ext, site, singlecell.steps + 1, intxn_matrix, rsamples[idx],\n",
    "                                                      beta=beta, ext_field=ext_field, app_field=app_field,\n",
    "                                                      ext_field_strength=ext_field_strength,\n",
    "                                                      app_field_strength=app_field_strength)\n",
    "        #print \"POST A\", singlecell.steps + 1, state_array_ext[site, singlecell.steps + 1]\n",
    "    singlecell.state_array = state_array_ext\n",
    "    singlecell.steps += 1\n",
    "    singlecell.state = state_array_ext[:, -1]\n",
    "    return singlecell\n",
    "\n",
    "\n",
    "def gpu_synced_update_step(current_state, intxn_matrix, beta=BETA, ext_field=None, app_field=None,\n",
    "                           ext_field_strength=EXT_FIELD_STRENGTH, app_field_strength=APP_FIELD_STRENGTH):\n",
    "    # copy state to gpu\n",
    "    N = current_state.shape[0]\n",
    "    gpu_current_state = torch.from_numpy(current_state).cuda()\n",
    "    # Step 1 - J x(t)\n",
    "    #print intxn_matrix.shape, gpu_current_state.shape\n",
    "    gpu_Jx = torch.mv(intxn_matrix, gpu_current_state)\n",
    "    # Step 2 - pointwise transform as 1/(1 + exp( -2 * beta * elem))\n",
    "    #gpu_transformed_Jx = torch.mul(gpu_Jx, -2.0*beta)\n",
    "    #gpu_transformed_Jx = torch.sigmoid(gpu_transformed_Jx)\n",
    "    gpu_transformed_Jx = torch.sigmoid(-2.0*beta*gpu_Jx)\n",
    "    \n",
    "    # Step 3 - pointwise comparison to rsamples U[0,1] vector (if elem - u > 0, then its 1.0, else -1.0)\n",
    "    gpu_transformed_Jx = torch.add(gpu_transformed_Jx, -torch.cuda.DoubleTensor(N).uniform_())\n",
    "    # Step 4 - convert to boolean -1, 1 using torch.sign\n",
    "    gpu_state_vec_next = torch.sign(gpu_transformed_Jx)\n",
    "    # Step 5 - send back to cpu\n",
    "    state_vec_next = gpu_state_vec_next.cpu().numpy()\n",
    "    #print type(state_vec_next), state_vec_next.shape, N\n",
    "    return state_vec_next\n",
    "\n",
    "\n",
    "def gpu_update_state_sync(singlecell, intxn_matrix, ext_field=None, ext_field_strength=EXT_FIELD_STRENGTH, \n",
    "                          beta=BETA, app_field=None, app_field_strength=APP_FIELD_STRENGTH, async_batch=ASYNC_BATCH):\n",
    "    \"\"\"\n",
    "    BATCHED evolution i.e. synchronous...\n",
    "    this can be like pointwise op W on x(t+1) = W ( Jx(t) ), W like 1/(1+np.exp(-2 beta elem))\n",
    "    then compare W ( Jx(t) ) elements vs unirand 0,1 to return 1 pr -1 for x(t+1)\n",
    "    \"\"\"\n",
    "    assert intxn_matrix.is_cuda\n",
    "    assert app_field is None and ext_field is None\n",
    "\n",
    "    current_state = singlecell.state_array[:,-1]\n",
    "    # update state here\n",
    "    state_vec_next = gpu_synced_update_step(current_state, intxn_matrix, beta=beta, ext_field=ext_field, \n",
    "                                            app_field=app_field, ext_field_strength=ext_field_strength, app_field_strength=app_field_strength)\n",
    "    # copy extend state array\n",
    "    state_array_ext = np.zeros((singlecell.N, np.shape(singlecell.state_array)[1] + 1))\n",
    "    state_array_ext[:, :-1] = singlecell.state_array  # TODO: make sure don't need array copy\n",
    "    state_array_ext[:,-1] = state_vec_next[:]\n",
    "    # update attributes\n",
    "    singlecell.state_array = state_array_ext\n",
    "    singlecell.steps += 1\n",
    "    singlecell.state = state_array_ext[:, -1]\n",
    "    \n",
    "    return singlecell\n",
    "\n",
    "    \n",
    "\n",
    "# TODO flag in simsetup to use method update state or gpu update state is all we need? and simsetup with gpu copy optionally at top\n",
    "def gpu_singlecell_sim(init_state=None, init_id=None, iterations=NUM_FULL_STEPS, beta=BETA, simsetup=None,\n",
    "                       gpu_simsetup=None, field_protocol=None, field_level=None, flag_burst_error=False, flag_write=True,\n",
    "                       analysis_subdir=None, plot_period=10, verbose=True):\n",
    "    \"\"\"\n",
    "    init_state: N x 1\n",
    "    init_id: None, or memory label like 'esc', or arbitrary label (e.g. 'All on')\n",
    "    iterations: main simulation loop duration\n",
    "    field_protocol: label for call field_setup to build field dict for applied field\n",
    "    flag_burst_error: if True, randomly flip some TFs at each BURST_ERROR_PERIOD (see ...constants.py)\n",
    "    flag_write: False only if want to avoid saving state to file\n",
    "    analysis_subdir: use to store data for non-standard runs\n",
    "    plot_period: period at which to plot cell state projection onto memory subspace\n",
    "    \"\"\"\n",
    "    # TODO: if dirs is None then do run subdir setup (just current run dir?)\n",
    "    # IO setup\n",
    "    if flag_write:\n",
    "        io_dict = run_subdir_setup(run_subfolder=analysis_subdir)\n",
    "    else:\n",
    "        if verbose:\n",
    "            print \"Warning: flag_write set to False -- nothing will be saved\"\n",
    "        io_dict = None\n",
    "\n",
    "    # simsetup unpack\n",
    "    if simsetup is None:\n",
    "        simsetup = singlecell_simsetup()\n",
    "    N, P, gene_labels, memory_labels, gene_id, celltype_id, xi, _, a_inv, intxn_matrix, _ = unpack_simsetup(simsetup)\n",
    "    gpu_intxn_matrix = simsetup['gpu_J']\n",
    "    \n",
    "    # Cell setup\n",
    "    N = xi.shape[0]\n",
    "    if init_state is None:\n",
    "        if init_id is None:\n",
    "            init_id = \"All_on\"\n",
    "            init_state = 1 + np.zeros(N)  # start with all genes on\n",
    "        else:\n",
    "            init_state = xi[:, celltype_id[init_id]]\n",
    "    singlecell = Cell(init_state, init_id, memories_list=memory_labels, gene_list=gene_labels)\n",
    "\n",
    "    # Input checks\n",
    "    field_dict = field_setup(simsetup, protocol=field_protocol, level=field_level)\n",
    "    assert not field_dict['time_varying']  # TODO not yet supported\n",
    "    app_field = field_dict['app_field']\n",
    "    app_field_strength = field_dict['app_field_strength']\n",
    "\n",
    "    # Simulate\n",
    "    for step in xrange(iterations-1):\n",
    "        if verbose:\n",
    "            print \"cell steps:\", singlecell.steps, \" H(state) =\", singlecell.get_energy(intxn_matrix=intxn_matrix)  # TODO need general intxn_matrix parent class\n",
    "        # apply burst errors\n",
    "        if flag_burst_error and step % BURST_ERROR_PERIOD == 0:\n",
    "            singlecell.apply_burst_errors()\n",
    "        # prep applied field TODO see if better speed to pass array of zeros and ditch all these if not None checks...\n",
    "        if flag_write:\n",
    "            if singlecell.steps % plot_period == 0:\n",
    "                fig, ax, proj = singlecell.plot_projection(a_inv, xi, use_radar=True, pltdir=io_dict['plotdatadir'])\n",
    "                fig, ax, proj = singlecell.plot_overlap(xi, use_radar=True, pltdir=io_dict['plotdatadir'])\n",
    "        #singlecell = gpu_update_state(singlecell, gpu_intxn_matrix, beta=beta, app_field=app_field, app_field_strength=app_field_strength, async_batch=ASYNC_BATCH)\n",
    "        singlecell = gpu_update_state_sync(singlecell, gpu_intxn_matrix, beta=beta, app_field=app_field, app_field_strength=app_field_strength, async_batch=ASYNC_BATCH)\n",
    "\n",
    "    # Write\n",
    "    if verbose:\n",
    "        print singlecell.get_current_state()\n",
    "    if flag_write:\n",
    "        if verbose:\n",
    "            print \"Writing state to file..\"\n",
    "        singlecell.write_state(io_dict['datadir'])\n",
    "    if verbose:\n",
    "        print io_dict['basedir']\n",
    "        print \"Done\"\n",
    "    return singlecell.get_state_array(), io_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Arg 'level' is None -- setting field level to 'level_1'\n",
      "cell steps: 0  H(state) = -1059.9999999999943\n",
      "cell steps: 1  H(state) = -999.2018245944055\n",
      "cell steps: 2  H(state) = -995.0844336955593\n",
      "cell steps: 3  H(state) = -955.211442761804\n",
      "cell steps: 4  H(state) = -976.683732362431\n",
      "cell steps: 5  H(state) = -994.9553650369086\n",
      "cell steps: 6  H(state) = -985.2867072890498\n",
      "cell steps: 7  H(state) = -975.7213998629531\n",
      "cell steps: 8  H(state) = -981.7208086571686\n",
      "cell steps: 9  H(state) = -976.6101500962332\n",
      "cell steps: 10  H(state) = -981.0348275653338\n",
      "cell steps: 11  H(state) = -970.7078200635472\n",
      "cell steps: 12  H(state) = -973.5016925959127\n",
      "cell steps: 13  H(state) = -983.4318256447925\n",
      "cell steps: 14  H(state) = -1007.7228141418176\n",
      "cell steps: 15  H(state) = -989.883352564922\n",
      "cell steps: 16  H(state) = -999.3357962311004\n",
      "cell steps: 17  H(state) = -985.6202484103844\n",
      "cell steps: 18  H(state) = -998.4487147881733\n",
      "cell steps: 19  H(state) = -983.0117038660318\n",
      "cell steps: 20  H(state) = -1000.4463499114033\n",
      "cell steps: 21  H(state) = -1000.5776027586953\n",
      "cell steps: 22  H(state) = -966.0172843514315\n",
      "cell steps: 23  H(state) = -991.5406311913694\n",
      "cell steps: 24  H(state) = -1000.9934206881538\n",
      "cell steps: 25  H(state) = -1000.1002604658183\n",
      "cell steps: 26  H(state) = -978.1437445049978\n",
      "cell steps: 27  H(state) = -995.9681321944468\n",
      "cell steps: 28  H(state) = -991.8317160458125\n",
      "cell steps: 29  H(state) = -988.4919647377951\n",
      "cell steps: 30  H(state) = -982.0308977985387\n",
      "cell steps: 31  H(state) = -983.4499937090552\n",
      "cell steps: 32  H(state) = -995.7144525226918\n",
      "cell steps: 33  H(state) = -1005.568799410554\n",
      "cell steps: 34  H(state) = -981.6740019506368\n",
      "cell steps: 35  H(state) = -1005.7267331059462\n",
      "cell steps: 36  H(state) = -998.3532333742645\n",
      "cell steps: 37  H(state) = -986.3277164308807\n",
      "cell steps: 38  H(state) = -985.2363298322124\n",
      "cell steps: 39  H(state) = -975.6337152332633\n",
      "cell steps: 40  H(state) = -989.818882363902\n",
      "cell steps: 41  H(state) = -994.0006499824997\n",
      "cell steps: 42  H(state) = -994.4500107116044\n",
      "cell steps: 43  H(state) = -989.9473761163132\n",
      "cell steps: 44  H(state) = -987.0688903738037\n",
      "cell steps: 45  H(state) = -959.445194001775\n",
      "cell steps: 46  H(state) = -988.0446096958948\n",
      "cell steps: 47  H(state) = -991.3245895460578\n",
      "cell steps: 48  H(state) = -985.4135506643775\n",
      "[ 1.  1.  1. ... -1. -1. -1.]\n",
      "Writing state to file..\n",
      "/media/homes/msmart/Development/repos/biomodels/celltypes/singlecell/runs/2019-01-12 01.18.02PM\n",
      "Done\n",
      "GPU timer: 11.0539469719\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "gpu_singlecell_sim(init_id='Macrophage (A)', field_protocol=None, plot_period=10,\n",
    "               iterations=50, simsetup=simsetup, flag_write=True, beta=2.2)\n",
    "deltat = time.time() - t0\n",
    "print \"GPU timer:\", deltat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix product example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expect shape of A*B^T to be m x m: (2000, 2000)\n",
      "First few elements:\n",
      "[[-9.538572 23.039274]\n",
      " [ 4.491309 12.267576]]\n",
      "Time: 0.339940786362 \n",
      "\n",
      "Trying vector dot product on GPU...\n",
      "Step 1: time = 0.00435900688171\n",
      "Step 1: types are <type 'numpy.ndarray'> <class 'torch.Tensor'>\n",
      "Step 2: time = 0.0176529884338\n",
      "Step 2: type is <class 'torch.Tensor'>\n",
      "Step 3: time = 0.000581026077271\n",
      "Step 4: time = 0.057755947113\n",
      "GPU: Expect shape of A*B^T to be m x m: torch.Size([2000, 2000])\n",
      "First few elements:\n",
      "tensor([[-9.5386, 23.0392],\n",
      "        [ 4.4913, 12.2676]])\n",
      "\n",
      "PyTorch non-cuda timing: time = 0.484139204025\n",
      "Torch CPU: Expect shape of A*B^T to be m x m: torch.Size([2000, 2000])\n",
      "First few elements:\n",
      "tensor([[-9.5385, 23.0392],\n",
      "        [ 4.4913, 12.2676]])\n",
      "\n",
      "GPU info:\n",
      "torch.cuda.memory_allocated() 896401408\n",
      "torch.cuda.memory_cached() 1681260544\n"
     ]
    }
   ],
   "source": [
    "# large random matrix dot product\n",
    "n = 5000\n",
    "m = 2000\n",
    "local_A = np.random.randn(n,m).astype('float32')\n",
    "local_B = np.random.randn(n,m).astype('float32')\n",
    "t0 = time.time()\n",
    "local_dot = np.dot(local_A.T, local_B)\n",
    "tdelta = time.time() - t0\n",
    "print \"Expect shape of A*B^T to be m x m:\", local_dot.shape\n",
    "print \"First few elements:\\n\", local_dot[0:2, 0:2]\n",
    "print \"Time:\", tdelta, '\\n'\n",
    "\n",
    "# now send data to gpu, compute, and return\n",
    "print \"Trying vector dot product on GPU...\"\n",
    "# STEP 1 - convert numpy to pytorch tensor\n",
    "t0 = time.time()\n",
    "torch_A = torch.from_numpy(local_A)\n",
    "torch_B = torch.from_numpy(local_B)\n",
    "print \"Step 1: time =\", time.time() - t0\n",
    "print \"Step 1: types are\", type(local_A), type(torch_B)\n",
    "# STEP 2 - send to gpu\n",
    "t0 = time.time()\n",
    "gpu_A = torch_A.cuda()\n",
    "gpu_B = torch_B.cuda()\n",
    "print \"Step 2: time =\", time.time() - t0\n",
    "print \"Step 2: type is\", type(gpu_A)\n",
    "# STEP 3 - compute\n",
    "t0 = time.time()\n",
    "gpu_dot = torch.mm(gpu_A.t(), gpu_B)\n",
    "print \"Step 3: time =\", time.time() - t0\n",
    "# STEP 4 - send back\n",
    "t0 = time.time()\n",
    "torch_A = gpu_A.cpu()\n",
    "torch_B = gpu_B.cpu()\n",
    "torch_dot = gpu_dot.cpu()\n",
    "print \"Step 4: time =\", time.time() - t0\n",
    "print \"GPU: Expect shape of A*B^T to be m x m:\", torch_dot.shape\n",
    "print \"First few elements:\\n\", torch_dot[0:2, 0:2]\n",
    "\n",
    "# and just cpu pytorch vs numpy\n",
    "t0 = time.time()\n",
    "torch_dot = torch.mm(torch_A.t(), torch_B)\n",
    "print \"\\nPyTorch non-cuda timing: time =\", time.time() - t0\n",
    "print \"Torch CPU: Expect shape of A*B^T to be m x m:\", torch_dot.shape\n",
    "print \"First few elements:\\n\", torch_dot[0:2, 0:2]\n",
    "\n",
    "print \"\\nGPU info:\"\n",
    "print \"torch.cuda.memory_allocated()\", torch.cuda.memory_allocated()\n",
    "print \"torch.cuda.memory_cached()\", torch.cuda.memory_cached()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Torch Unirand vector on GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.1793, -0.4243, -0.7067, -1.2775, -1.7694], device='cuda:0')\n",
      "0.00175309181213\n",
      "0.000638961791992\n"
     ]
    }
   ],
   "source": [
    "N = 1000\n",
    "\n",
    "t0 = time.time()\n",
    "b = torch.cuda.FloatTensor(N).uniform_()\n",
    "c = 4*torch.mul(b, -1.0)\n",
    "print c.data[0:5]\n",
    "print time.time()-t0\n",
    "\n",
    "\n",
    "t0 = time.time()\n",
    "a = torch.rand(N, 1)\n",
    "b = a.cuda()\n",
    "c = torch.mul(b, -1.0)\n",
    "print time.time()-t0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
